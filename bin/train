#!/usr/bin/env ruby
# frozen_string_literal: true

$stdout.sync = true

# Train a GPT model
# Usage:
#   bundle exec ruby bin/train [options]
#   bundle exec ruby bin/train --config=config/train_shakespeare.json
#   bundle exec ruby bin/train --config=config/train_gpt2.json --max_iters=1000

require_relative "../lib/nano_gpt"

def main
  config = NanoGPT::TrainConfig.load(ARGV)

  # Resolve device (auto-detect if "auto")
  if config[:device] == "auto"
    config[:device] = NanoGPT::Device.auto
    puts "Auto-detected device: #{config[:device]}"
  end

  # Load tokenizer to get vocab_size (auto-detects char-level vs GPT-2 BPE)
  data_dir = File.join("data", config[:dataset])
  train_bin = File.join(data_dir, "train.bin")

  unless File.exist?(train_bin)
    puts "Error: #{train_bin} not found. Run the data preparation script first:"
    puts "  bundle exec ruby data/#{config[:dataset]}/prepare.rb"
    exit 1
  end

  tokenizer = NanoGPT::Tokenizer.for_dataset(data_dir)
  tokenizer_type = tokenizer.is_a?(NanoGPT::GPT2Tokenizer) ? "GPT-2 BPE" : "character-level"
  puts "Loaded #{tokenizer_type} tokenizer with vocab_size=#{tokenizer.vocab_size}"

  # Create model config
  model_config = NanoGPT::GPTConfig.new(
    block_size: config[:block_size],
    vocab_size: tokenizer.vocab_size,
    n_layer: config[:n_layer],
    n_head: config[:n_head],
    n_embd: config[:n_embd],
    dropout: config[:dropout],
    bias: config[:bias]
  )

  # Create model
  model = NanoGPT::GPT.new(model_config)

  # Move model to device (MPS, CUDA, or CPU)
  device = config[:device]
  if device != "cpu"
    model.to(device)
    puts "Model moved to #{device}"
  end

  # Create data loader
  data_loader = NanoGPT::DataLoader.new(
    data_dir: data_dir,
    block_size: config[:block_size],
    batch_size: config[:batch_size],
    device: config[:device]
  )

  puts "Train data: #{data_loader.train_size} tokens"
  puts "Val data: #{data_loader.val_size} tokens"

  # Create trainer
  trainer = NanoGPT::Trainer.new(
    model: model,
    data_loader: data_loader,
    config: config.to_h
  )

  # Train!
  trainer.train

  puts "\nTraining complete! Checkpoint saved to #{config[:out_dir]}/ckpt.pt"
end

main
