#!/usr/bin/env ruby
# frozen_string_literal: true

# Generate text from a trained model
# Usage: bundle exec ruby bin/sample [options]

require_relative "../lib/nano_gpt"

CONFIG = {
  out_dir: "out-shakespeare-char",
  dataset: "shakespeare_char",
  start: "\n",
  num_samples: 5,
  max_new_tokens: 500,
  temperature: 0.8,
  top_k: 200,
  seed: 1337,
  # Use "auto" to auto-detect best device (MPS on Mac, CUDA on Linux, CPU otherwise)
  device: "auto"
}.freeze

def parse_args(args, config)
  result = config.dup

  args.each do |arg|
    next unless arg.start_with?("--") && arg.include?("=")

    key, val = arg[2..].split("=", 2)
    key = key.to_sym

    unless result.key?(key)
      puts "Warning: Unknown config key: #{key}"
      next
    end

    result[key] = case result[key]
                  when Integer then val.to_i
                  when Float then val.to_f
                  when TrueClass, FalseClass then val.downcase == "true"
                  else val
                  end
  end

  result
end

def main
  config = parse_args(ARGV, CONFIG)

  # Resolve device (auto-detect if "auto")
  if config[:device] == "auto"
    config[:device] = NanoGPT::Device.auto
    puts "Auto-detected device: #{config[:device]}"
  end
  device = config[:device]

  Torch.manual_seed(config[:seed])

  # Load checkpoint
  ckpt_path = File.join(config[:out_dir], "ckpt.pt")
  unless File.exist?(ckpt_path)
    puts "Error: Checkpoint not found at #{ckpt_path}"
    puts "Train a model first with: bundle exec ruby bin/train"
    exit 1
  end

  puts "Loading checkpoint from #{ckpt_path}..."
  checkpoint = Torch.load(ckpt_path)

  # Recreate model from checkpoint
  model_args = checkpoint["model_args"].transform_keys(&:to_sym)
  model_config = NanoGPT::GPTConfig.new(**model_args)
  model = NanoGPT::GPT.new(model_config)
  model.load_state_dict(checkpoint["model"])
  model.to(device) if device != "cpu"
  model.eval

  # Load tokenizer (auto-detects character-level vs GPT-2 BPE)
  dataset_dir = File.join("data", config[:dataset])
  tokenizer = NanoGPT::Tokenizer.for_dataset(dataset_dir)
  puts "number of parameters: #{model.num_params / 1e6}M"

  # Handle start text
  start_text = config[:start]
  if start_text.start_with?("FILE:")
    start_text = File.read(start_text[5..])
  end

  # Encode starting prompt
  start_ids = tokenizer.encode(start_text)
  x = Torch.tensor([start_ids], dtype: :long, device: device)

  puts "Generating #{config[:num_samples]} samples..."
  puts "=" * 50

  # Generate samples
  config[:num_samples].times do |k|
    y = model.generate(
      x,
      config[:max_new_tokens],
      temperature: config[:temperature],
      top_k: config[:top_k]
    )

    output = tokenizer.decode(y[0].to_a)
    puts output
    puts "-" * 50
  end
end

main
